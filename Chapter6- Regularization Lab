
########    Applied Questions-8   #############
1. Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector  of length n = 100.
2. Generate a response vector Y of length n=100 a/c to the model Y = β0 + β1X + β2X^2 + β3X^3 + eps, choose beta coefficient on ur own
3. Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictorsX,X2, . 
   . .,X10. What is the best model obtained according to Cp, BIC, and adjusted R2?
4. Repeat (3) using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the
    results in (3)?
5. Now fit a lasso model to the simulated data, again using X,X2, . . . , X10 as predictors. Use cross-validation to select the optimal
  value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coefficient estimates, and discuss
  the results obtained.

#1. Creating a dataframe with simulation
set.seed(1)
X = rnorm(100)
eps = rnorm(100)

#2. Generate a response vector Y of length n = 100 according to the model Y = β0 + β1X + β2X^2 + β3X^3 + eps,
#where β0, β1, β2, and β3 are constants of your choice. We are selecting β0=3, β1=2, β2=−3 and β3=0.3.
beta0 = 3
beta1 = 2
beta2 = -3
beta3 = 0.3
Y = beta0 + beta1*X + beta2*X^2 + beta3*X^3 + eps

data1 = data.frame(y=Y, x=X) #Creating the data frame of X & Y

#best subset selection method
#Use regsubsets to select best model having polynomial of X of degree 10
library(leaps)
Data.Sub = regsubsets(y ~ poly(x,10, raw=T), data=data1, nvmax=10)

#What is the best model obtained according to cp, adjusted R squared & BIC
summary(Data.Sub)
Sub.Summary = summary(Data.Sub)
which.min(Sub.Summary$cp)  #3 variable model has minimum cp
which.max(Sub.Summary$adjr2)  # 3 variable model has maxmimum adsjuted R squared
which.min(Sub.Summary$bic)   #3 variable model has mnimum bic
Sub.Summary$cp #Values of CP for different variables
Sub.Summary$adjr2


#Clik on the brush in plot bar of RStudio if plot is showing error
#regsubsets() function has a built-in plot() command which can be used to display the selected
#variables for the best model with given no. of predictors, on the basis of various error parameters
plot(Data.Sub, scale="r2") #Here each black square shows that the particular variable is selected in model
plot(Data.Sub, scale = "Cp")
plot(Data.Sub, scale = "adjr2")


########## Forward & Backward Model Selection        #############

#forward selection method
for.Sub = regsubsets(y ~ poly(x,10, raw=T), data=data1, nvmax=10, method = "forward")
for.Summary = summary(for.Sub)
which.min(for.Summary$cp)     # 3 variable model has minimum cp
which.max(for.Summary$adjr2)  # 3 variable model has maxm adjusted R-squared
which.min(for.Summary$bic)  # 3 variable model has minimum bic


#backward selection method
back.Sub = regsubsets(y ~ poly(x,10, raw=T), data=data1, nvmax=10, method = "backward")
back.Summary = summary(back.Sub)
which.min(back.Summary$cp)     #3 variable model has minimum cp
which.max(back.Summary$adjr2)  #4 variable model has maxm adjusted R-squared
which.min(back.Summary$bic)  #3 variable model has minimum bic

#We see that both bic & cp choose 3-variable model while adjsuted R^2 chooses 7-var model using three techniques
coefficients(Data.Sub, id =3) #cofficients of optimal model in best subset selection
coefficients(for.Sub, id =3)  #cofficients of optimal model in forward selection
coefficients(back.Sub, id =3)  #cofficients of optimal model in backward selection
coefficients(back.Sub, id =4) #cofficients of optimal model in backward selection using adjusted R-sqaured

###    CREATING A LASSO REGRESSION ON THE SAME DATASET       #############

library(glmnet)
xmat = model.matrix(y ~ poly(x, 10, raw = T), data = data1)[,-1] #[,-1] will remove the intercept
Reg.lasso = cv.glmnet(xmat, Y, alpha = 1)
names(Reg.lasso)
best.lambda = Reg.lasso$lambda.min
best.lambda  #Value of best lamda is 0.03636829
plot(Reg.lasso)


# Next fit the model on entire data using best lambda
best.model = glmnet(xmat, Y, alpha = 1)
predict(best.model, s = best.lambda, type = "coefficients")
#So lasso pics x, x^2, x^5 & x^7. It chooses four variables where as backward & forward chooses 3 variable model with BIC & cp and 
4 variable model for adjusted R- squared

